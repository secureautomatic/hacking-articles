{"id":"https://thehackernews.com/2025/01/new-ai-jailbreak-method-bad-likert.html","title":"New AI Jailbreak Method 'Bad Likert Judge' Boosts Attack Success Rates by Over 60%","link":"https://thehackernews.com/2025/01/new-ai-jailbreak-method-bad-likert.html","published":1735902840000,"description":"Cybersecurity researchers have shed light on a new jailbreak technique that could be used to get past a large language model's (LLM) safety guardrails and produce potentially harmful or malicious responses. The multi-turn (aka many-shot) attack strategy has been codenamed Bad Likert Judge by Palo Alto Networks Unit 42 researchers Yongzhe Huang, Yang Ji, Wenjun Hu, Jay Chen, Akshata Rao, and","enclosure":{"url":"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh53on30HjbmoMag2FnIBQ5wzXH0y4kLfUZdnc5lyayjbSzYsHJAkQO-d2HYrjngF4ljiKz5edAHMKfkTsqmJ2fwXkxKUainRP2ppXjfxlF7tn-kaPTKQZnivateDHl-eOSSLvbEQjRLXy7wofsXCGGTbcZAykum_p3Iuah1aT1bQ1L_Cy4rXLh1tPdDmjo/s1600/ai-jailbreak.png","type":"image/jpeg","length":"12216320"},"pubDate":"Fri, 03 Jan 2025 16:44:00 +0530"}